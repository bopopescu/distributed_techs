ActiveJob[jobId,finalStage,numPartitions,finishedPartitions] a running job in DAGScheduler
	+ result job: computes the map outputs for a ShuffleMapStage before any downstream stages are submitted
	+ map-stage job: execute an action
1、ResultJob和MapStageJob两种类型的作业通过在finalStage字段判断，若为ResultStage则对应的作业即为ResultJob。
2、任何一种类型的作业都会导致Stage DAG图中比较靠前的Stage的执行（因为容错），同时多个作业的前向Stages可以共享。
3、finalStage:Stage，表示当前作业要执行计算的Stage，当为ResultStage时就会执行Action操作，当为ShuffleMapStage时就会调用DAGScheduler.submitMapStage执行某一个Stage。

JobResult[] a result of a job in DAGScheduler
	+ case object JobSucceeded
	+ case class JobFailed(exception)

trait JobListener {
	def taskSucceeded(index:Int,result: Any)
	def jobFailed(exception: Exception)
}
JobWaiter[dagScheduler,jobId,totalTasks,resultHandler:(Int,T)=>Unit] 
1、JobWaiter实现接口JobListener，自定义Job完成后的回调行为用于在任务成功或失败后创建JobSucceeded或JobFailed结果；
2、JobWaiter对象等待DAGScheculer Job完成，并调用resultHandler处理作业执行后的结果；
3、JobWaiter实现JobListener中接口，当作业执行完成后返回JobSucceeded作业结果，否则返回JobFailed结果；
4、JobWaiter.awaitResult方法内部循环检查任务是否完成，若没有则调用this.wait方法等待，在任务完成（taskSucceeded或jobFailed方法得到创建JobResult后调用this.notifyAll）返回JobResult。


Stage[id,rdd,numTasks,parentStages,firstJobId,callSite]
	+ ShuffleMapStage[id,rdd,numTasks,parentStages,firstJobId,callSite] produce data for a shuffle, occurs right before each shuffle operation and contain multiple pipelined operations
	+ ResultStage[id,rdd,func,partitions,parentStages,firstJobId,callSite] apply a function on some partitions of rdd to compute result of an action
StageInfo[stageId,stageAttemptId,name,numTasks,rddInfos,parentIds,detail,taskLocalityPreferences] stores info about a stage to pass from scheduler to SparkListeners
说明：
1、DAGScheduler根据shuffle依赖划分Stage，并根据Stage图的拓扑排序顺序执行每一个Stage，每一个Stage中的Tasks由TaskScheduler负责执行。
2、一个Stage内部是多个可以并行运行的Tasks，这些Tasks执行操作、shuffle依赖相同只是操作的数据不同。
3、Stage有两类：ShuffleMapStage和ResultStage，ShuffleMapStage中Tasks的结果为其他Stage的输入，ResultStage中的Tasks的结果为执行RDD Action(如：count)计算后的结果。
4、每个Stage关联一个firstJobId，用于标识第一次提交Stage的作业。
5、Stage支持容错，允许Stage连续执行失败4次，超过4次仍然执行失败则放弃该stage。
6、ShuffleMapStage为DAG图中中间过程，主要维护Stage执行结果的保存位置、关联的Job信息，并根据这些信息判断哪些分区没有执行成功。
7、ResultStage内部保存关联的作业，并根据作业的结果判断哪些分区没有执行成功，若作业已经执行完成则关联作业为空。

Task[stageId,stageAttemptId,partitionId] a unit of execution
	+ ShuffleMapTask[stageId,stageAttemptId,taskBinary,partition,taskLocations,internalAccumlators] executes task and divides task output to multiple buckets based on task's partitioner 
	+ ResultTask[stageId,stageAttemptId,taskBinary,partition,taskLocations,outputId,internalAccumlators] executes task and sends task output to driver
说明：
1、taskBinary元组(RDD[T], (TaskContext,Iterator[T])=>U)第一值为序列化后的RDD，第二个值为应用于所有RDD分区上的操作。
2、真正任务的执行是在ResultTask.runTask中开始的并在执行完成后返回执行结果。
3、ShuffleMapTask.runTask方法内部使用SparkEnv.ShuffleManager.write(partition.iterator)将分区数据写到不同外部存储中。
4、ShuffleMapTask的返回结果为MapStatus的集合。

MapStatus[BlockManagerId]
	+ CompressedMapStatus[BlockManagerId,compressedSizes]
	+ HighlyCompressedMapStatus[BlockManagerId,numNonEmptyBlocks,emptyBlocks,avgSize]
1、MapStatus是ShuffleMapTask的返回结果，需要报送给TaskScheduler，包括该任务执行所在的BlockManager的地址以及该任务为每个Reducer产生的数据的大小（会传递给reduce task）；
2、MapStatus中有一个数组存放该ShuffleMapTask为每个Reducer产生的数据量大小，当Reducer较多时会导致数组较大因此会采用压缩算法将整形数组进行压缩；
3、当Reducer数大于2000时采用基于log的压缩算法CompressedMapStatus，否则采用基于bitmap压缩算法HighlyCompressedMapStatus作为返回结果；
4、为什么MapStatus中保存着为每一个Reduce Task生成的数据量的大小？

TaskResult
	+ DirectTaskResult(valueBytes, accumUpdates)task return values that stored in memory
	+ IndirectTaskResult(blockid, size) an reference to DirectTaskResult that has been stored in worker's BlockManager
TaskResultGetter:采用线程池(spark.resultGetter.threads=4),根据TaskResult类型分别从内存或磁盘获取数据并反序列化后返回，若结果大小超过maxResultSize则直接丢弃。

TaskInfo[taskId,index,attemptNumber,launchTime,executorId,host,taskLocality, speculative,finishTime,failed] a running task attempt inside a taskset
TaskDescription[taskid, attemptNumber, executorId, name, index, _serializedTask] description of a task that gets passed onto executors to be executed,generated by TaskSetManager.resourceOffer
TaskLocality[PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY]
TaskLocation[host] a location where a task should run
	+ HostTaskLocation[host]
	+ HDFSCacheTaskLocation[host] Tag-->"hdfs_cache_${host}"
	+ ExecutorCacheTaskLocation[host, executorid] Tag-->"executor_${host}"

WorkerOffer[host, cores] avaiable resources
